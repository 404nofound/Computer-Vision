# 卷积神经网络学习笔记

转载整理自[通俗理解卷积神经网络](https://blog.csdn.net/v_JULY_v/article/details/51812459)

## 人工神经网络
**神经元**  

神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为权重（weight）。不同的权重和激活函数，则会导致神经网络不同的输出。


![neuron](media/16108799095604/neuron.png)


基本wx + b的形式，其中

* x1, x2表示输入向量
* w1, w2为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重
* b为偏置bias
* g(z) 为激活函数
* a 为输出

举例：
```
有一场音乐节，去或不去？

x1是否有喜欢的演唱嘉宾。x1=1为你喜欢这些嘉宾，x1=0为你不喜欢这些嘉宾。嘉宾因素的权重=7

x2是否有人陪你同去。x2=1为有人陪你同去，x2=0为没人陪你去。是否有人陪同的权重=3

决策模型便建立起来了：g(z) = g(w1*x1 + w2*x2 + b)，
g表示激活函数，这里的b可以理解成: 为更好达到目标而做调整的偏置项。
```

## 激励函数

常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。

激励函数详细介绍可参考[神经网络常用激励函数]()

## 神经网络结构

![layer-w720](media/16108799095604/layer.jpeg)

## 卷积运算实例

